{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity laws obeyed constructing aeroelastic models heated high speed aircraft .\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(query):\n",
    "    stopwords = ['a', 'the', 'an', 'and', 'or', 'but', 'about', 'above', 'after', 'along', 'amid', 'among', 'as', 'at', 'by', 'for', 'from', 'in', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'out', 'over', 'past', 'per', 'plus', 'since', 'till', 'to', 'up', 'via', 'vs', 'with', 'that', 'can', 'cannot', 'could', 'may', 'might', 'must', 'need', 'ought', 'shall', 'should', 'will', 'would', 'have', 'had', 'has', 'having', 'be', 'is', 'am', 'are', 'was', 'were', 'being', 'been', 'get', 'gets', 'got', 'gotten', 'getting', 'seem', 'seeming', 'seems', 'seemed', 'enough', 'both', 'all', 'your', 'those', 'this', 'these', 'their', 'the', 'that', 'some', 'our', 'no', 'neither', 'my', 'its', 'his', 'her', 'every', 'either', 'each', 'any', 'another', 'an', 'a', 'just', 'mere', 'such', 'merely', 'right', 'no', 'not', 'only', 'sheer', 'most', 'rather', 'somewhat', 'sufficiently', 'same', 'different', 'such', 'when', 'why', 'where', 'how', 'what', 'who', 'whom', 'which', 'whether', 'why', 'whose', 'if', 'anybody', 'anyone', 'anyplace', 'anything', 'anytime', 'anywhere', 'everybody', 'everyday', 'everyone', 'everyplace', 'everything', 'everywhere', 'whatever', 'whenever', 'wherever', 'whichever', 'whoever', 'whomever', 'he', 'him', 'his', 'her', 'she', 'it', 'they', 'them', 'its', 'their', 'theirs', 'you', 'your', 'yours', 'me', 'my', 'mine', 'I', 'we', 'us', 'much', 'and/or']\n",
    "\n",
    "    # Convertendo para minúsculas \n",
    "    cleaned_query = query.lower()\n",
    "    \n",
    "    # Separando as palavras da consulta\n",
    "    words = cleaned_query.split()\n",
    "    \n",
    "    # Removendo stopwords da consulta\n",
    "    cleaned_words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # Reconstruindo a consulta sem as stopwords\n",
    "    cleaned_query = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "\n",
    "# Defina o esquema dos documentos\n",
    "schema = Schema(id=ID(unique=True, stored=True),\n",
    "                title=TEXT(stored=True),\n",
    "                author=TEXT(stored=True),\n",
    "                bibliography=TEXT(stored=True),\n",
    "                content=TEXT(stored=True))\n",
    "\n",
    "# Crie um índice Whoosh\n",
    "index_dir = \"C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\whoosh\"\n",
    "ix = create_in(index_dir, schema)\n",
    "    \n",
    "\n",
    "# C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\cran.all.1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ix.writer()\n",
    "current_document = {}\n",
    "with open('C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\cran.all.1400', 'r') as file:\n",
    "    documents = []\n",
    "    current_document = {}\n",
    "    content_started = False  # Indicador para começar a ler o conteúdo do documento\n",
    "    line = file.readline()\n",
    "    \n",
    "    while line:  \n",
    "        if line.startswith(\".I\"):\n",
    "            if current_document:\n",
    "                # Adiciona o documento atual à lista de documentos\n",
    "                writer.add_document(**current_document)\n",
    "                current_document = {}  # Limpa as informações do documento atual\n",
    "\n",
    "            current_document['id'] = line.split()[1].strip()  # Obter o ID do documento\n",
    "        \n",
    "        try:\n",
    "            line = next(file).strip()  # Lê a próxima linha\n",
    "        except StopIteration:\n",
    "            break  # Se acabaram as linhas, pare\n",
    "        \n",
    "        if line.startswith(\".T\"):\n",
    "            content_started = True\n",
    "            current_document['title'] = \"\"  # Inicializa o título do documento\n",
    "            line = next(file).strip()  # Pula para a próxima linha após a tag\n",
    "\n",
    "            while line and not line.startswith((\".I\", \".T\", \".A\", \".B\", \".W\")):\n",
    "                current_document['title'] +=  remove_stopwords(line) + \" \"  # Lê o título do documento\n",
    "                try:\n",
    "                    line = next(file).strip()  # Lê a próxima linha\n",
    "                except StopIteration:\n",
    "                    break  # Se acabaram as linhas, pare\n",
    "\n",
    "        if line.startswith(\".A\"):\n",
    "            content_started = True\n",
    "            current_document['author'] = \"\"  # Inicializa o autor do documento\n",
    "            line = next(file).strip()  # Pula para a próxima linha após a tag\n",
    "\n",
    "            while line and not line.startswith((\".I\", \".T\", \".A\", \".B\", \".W\")):\n",
    "                current_document['author'] += line + \" \" # Lê o autor do documento\n",
    "                try:\n",
    "                    line = next(file).strip()  # Lê a próxima linha\n",
    "                except StopIteration:\n",
    "                    break  # Se acabaram as linhas, pare\n",
    "                \n",
    "        if line.startswith(\".B\"):\n",
    "            content_started = True\n",
    "            current_document['bibliography'] = \"\"  # Inicializa o conteúdo do documento\n",
    "            line = next(file).strip()  # Pula para a próxima linha após a tag\n",
    "\n",
    "            while line and not line.startswith((\".I\", \".T\", \".A\", \".B\", \".W\")):\n",
    "                current_document['bibliography'] +=  line  + \" \"  # Lê o conteúdo do documento\n",
    "                try:\n",
    "                    line = next(file).strip()  # Lê a próxima linha\n",
    "                except StopIteration:\n",
    "                    break  # Se acabaram as linhas, pare\n",
    "\n",
    "        if line.startswith(\".W\"):\n",
    "            content_started = True\n",
    "            current_document['content'] = \"\"  # Inicializa o conteúdo do documento\n",
    "            line = next(file).strip()  # Pula para a próxima linha após a tag\n",
    "\n",
    "            while line and not line.startswith((\".I\", \".T\", \".A\", \".B\", \".W\")):\n",
    "                current_document['content'] +=  remove_stopwords(line) + \" \"\n",
    "                # Lê o conteúdo do documento\n",
    "                try:\n",
    "                    line = next(file).strip()  # Lê a próxima linha\n",
    "                except StopIteration:\n",
    "                    break  # Se acabaram as linhas, pare\n",
    "        \n",
    "        \n",
    "    # Adiciona o último documento à lista de documentos\n",
    "    if current_document:\n",
    "        writer.add_document(**current_document)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import open_dir\n",
    "from whoosh.qparser import MultifieldParser\n",
    "\n",
    "def busca(original_query):\n",
    "    # Abrir o índice dos documentos\n",
    "    document_index = open_dir(\"C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\whoosh\")\n",
    "\n",
    "    # Definir os campos onde deseja pesquisar\n",
    "    fields = [\"content\"]\n",
    "\n",
    "    # Criar um parser de consulta para pesquisar em vários campos\n",
    "    query_parser = MultifieldParser(fields, schema=document_index.schema)\n",
    "\n",
    "    # Remover stopwords, se necessário\n",
    "    query_without_stopwords = remove_stopwords(original_query)\n",
    "\n",
    "    # Substituir espaços por \" OR \"\n",
    "    query_with_OR = query_without_stopwords.replace(\" \", \" OR \")\n",
    "\n",
    "    query = query_parser.parse(query_with_OR)\n",
    "\n",
    "    # Realizar a busca\n",
    "    with document_index.searcher() as searcher:\n",
    "        results = searcher.search(query)\n",
    "\n",
    "        if len(results) > 0:\n",
    "            #print(\"Documentos encontrados:\")\n",
    "            found_ids = []  # Lista para armazenar os IDs encontrados\n",
    "            for result in results:\n",
    "                #print(f\"ID: {result['id']} - Score: {result.score}\")\n",
    "                found_ids.append(int(result['id']))  # Adiciona o ID encontrado à lista\n",
    "                #print(\"\\n\")\n",
    "            return found_ids  # Retorna a lista de IDs encontrados\n",
    "        else:\n",
    "            #print(\"Nenhum documento encontrado para esta consulta.\")\n",
    "            return []  # Retorna uma lista vazia, já que nenhum documento foi encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_from_cranqry(file_path):\n",
    "    queries = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        current_query_id = None\n",
    "        current_query = None\n",
    "        for line in lines:\n",
    "            if line.startswith('.I'):\n",
    "                if current_query_id and current_query:\n",
    "                    queries[current_query_id] = remove_stopwords(current_query) + \" \"\n",
    "                current_query_id = line.split()[1].strip()\n",
    "                current_query = ''\n",
    "            elif line.startswith('.W'):\n",
    "                continue\n",
    "            else:\n",
    "                current_query += line.strip() + ' '\n",
    "        # Adicionar a última consulta\n",
    "        if current_query_id and current_query:\n",
    "            queries[current_query_id] = remove_stopwords(current_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_documents(file_path):\n",
    "    relevant_docs = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            query_id, doc_id, relevance = map(int, line.strip().split())\n",
    "            if query_id not in relevant_docs:\n",
    "                relevant_docs[query_id] = []\n",
    "            relevant_docs[query_id].append((doc_id, relevance))\n",
    "\n",
    "    # Ordenar os documentos relevantes e manter somente os 10 primeiros\n",
    "    for query_id, docs in relevant_docs.items():\n",
    "        sorted_docs = sorted(docs, key=lambda x: x[1], reverse=True)[:10]\n",
    "        relevant_docs[query_id] = [doc[0] for doc in sorted_docs]\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Média =  1.6118421052631584\n",
      "Tempo total de execução das buscas: 2.5390 segundos\n"
     ]
    }
   ],
   "source": [
    "from whoosh.index import open_dir\n",
    "from whoosh.qparser import MultifieldParser\n",
    "import time\n",
    "\n",
    "counter = 0\n",
    "media = 0\n",
    "\n",
    "# Caminho para o arquivo cranqrel\n",
    "cranqrel_file_path = 'C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\cranqrel'\n",
    "\n",
    "# Extrair documentos relevantes do arquivo cranqrel\n",
    "relevant_documents = extract_relevant_documents(cranqrel_file_path)\n",
    "    \n",
    "# Caminho para o arquivo cran.qry\n",
    "qry_file_path = 'C:\\\\Users\\\\maxwe\\\\OneDrive\\\\Área de Trabalho\\\\cran\\\\cran.qry'\n",
    "\n",
    "# Extrair as consultas do arquivo cran.qry\n",
    "queries = extract_queries_from_cranqry(qry_file_path)\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id, query_text in queries.items():\n",
    "    if int(query_id) > 225:\n",
    "        break\n",
    "    counter = counter + 1\n",
    "    \n",
    "    \n",
    "    results = busca(query_text)\n",
    "\n",
    "    \n",
    "    relevant_docs_found = results  # A lista de IDs encontrados já é retornada por busca()\n",
    "\n",
    "    relevant_docs_current_query = relevant_documents[int(query_id.lstrip('0'))]\n",
    "\n",
    "    # print(f\"Consulta ID: {query_id}\")\n",
    "    # print(f\"Documentos Relevantes: {relevant_docs_current_query}\")\n",
    "    # print(f\"Documentos Encontrados na Busca: {relevant_docs_found}\")\n",
    "\n",
    "    # Converter relevant_docs_found para um conjunto para facilitar a comparação\n",
    "    relevant_docs_found_set = set(relevant_docs_found)\n",
    "\n",
    "    # Encontrar a interseção entre os documentos relevantes da consulta e os encontrados na busca\n",
    "    common_docs = relevant_docs_found_set.intersection(relevant_docs_current_query)\n",
    "    \n",
    "    media = media + len(common_docs)/len(relevant_documents[int(query_id.lstrip('0'))])\n",
    "\n",
    "    # print(f\"Total de Documentos Relevantes para esta consulta: {len(relevant_docs_current_query)}\")\n",
    "    # print(f\"Documentos Encontrados na Busca: {len(relevant_docs_found)}\")\n",
    "    #print(f\"Documentos Relevantes Encontrados na Busca: {len(common_docs)}\")\n",
    "    #print(\"\\n\")\n",
    "end_time = time.time()   \n",
    "print(\"Acurácia Média = \",(media/counter)*100)\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tempo total de execução das buscas: {execution_time:.4f} segundos\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
